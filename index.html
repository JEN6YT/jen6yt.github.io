<!DOCTYPE html>
<html lang="en">    

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="styles.css">
    <script src="script.js" defer></script>
</head>

<body>
    <header>
        <nav class="nav">
            <div class="logo">
                <a href="#about" data-link>@jenniferzhang 张云童</a>
            </div>
            <button class="nav-toggle" aria-label="Toggle navigation">☰</button>
            <div class="nav-links">
                <a href="#about" data-link>About</a>
                <a href="#projects" data-link>Publications</a>
                <a href="#cv" data-link>CV</a>
                <a href="#building" data-link>Building</a>
                <button id="dark-mode-toggle" aria-label="Toggle dark mode">&#9790;</button>
            </div>
        </nav>
    </header>

    <main>
        <section id="about">
            <div class="about-header">
                <div class="about-text">
                    <p>Hi! I am Jennifer. I study
                        <a href="https://engsci.utoronto.ca/" target="_blank">Engineering Science</a> at University of Toronto. 
                        During this program, I have learnt about engineering disciplines, programming skills, and various interdisciplinary topics.
                        In addition, I have developed a strong interest in data science and machine learning.
                        My current research focuses on <b>Large Foundation Models Alignment</b>, <b>Reinforcement Learning</b>, and <b>Agent-Human Alignment</b> (multi-agent interactions and alignment).
                        I am deeply interested in using AI to solve real-world problems, and I am particularly fascinated by the potential of AI to enhance human capabilities, and for social good.
                        I am fortunate to be guided by Professor <a href="https://cseweb.ucsd.edu/~jmcauley/" target="_blank">Julian McAuley</a>, Professor <a href="https://www.mccormick.northwestern.edu/research-faculty/directory/profiles/liu-han.html" target="_blank">Han Liu</a> 
                        Professor <a href="https://vectorinstitute.ai/team/william-cunningham/">William Cunningham</a>, and Professor <a href="https://seco.risklab.ca/">Luis Seco</a> on various research projects.
                    </p>
                    <p>In my free time, I enjoy outdoor activities and all kinds of sports. I love exploring new technologies: you can see my blog posts <a href="https://medium.com/@jennifer.ytzhang">here</a>. If you would love to have a chat, feel free to shoot me an email! </p>
                </div>
                <div class="about-image">
                    <img src="./assets/jennifer.jpg" alt="Jennifer Zhang" />

                    <div class="icons">
                        <a href="mailto:jenniferyt.zhang@mail.utoronto.ca" target="_blank" aria-label="Email">
                            <img src="https://cdn.jsdelivr.net/gh/simple-icons/simple-icons/icons/gmail.svg" alt="Email" />
                        </a>
                        <a href="https://github.com/JEN6YT" target="_blank" aria-label="GitHub">
                            <img src="https://cdn.jsdelivr.net/gh/simple-icons/simple-icons/icons/github.svg" alt="GitHub" />
                        </a>
                        <a href="https://www.linkedin.com/in/jennifer-yuntong-zhang-a9225b21b/" target="_blank" aria-label="LinkedIn">
                            <img src="https://cdn.jsdelivr.net/gh/simple-icons/simple-icons/icons/linkedin.svg" alt="LinkedIn" />
                        </a>
                        <a href="https://scholar.google.com/citations?user=1GrCDH8AAAAJ&hl=en" target="_blank" aria-label="Scholar">
                            <img src="https://cdn.jsdelivr.net/gh/simple-icons/simple-icons/icons/googlescholar.svg" alt="Scholar" />
                        </a>
                    </div>
                </div>
            </div>
            <img src="assets/sunflower.png" alt="Sunflower" class="decorative-image" />
        </section>

        <section id="projects">
            <div class="project-entry">
                <h3>In-Context Algorithm Emulation in Fixed-Weight Transformers</h3>
                <div class="project-callout">
                    <p class="callout-authors">
                        J. Y.-C. Hu<sup class="big-star">*</sup>, H. Liu<sup class="big-star">*</sup>, <strong>J. Y. Zhang<sup class="big-star">*</sup></strong>, H. Liu<br>
                        In submission to ICLR 2026.
                        <sup class="big-star">*</sup> Equal contribution.
                    </p>
                    <div class="project-actions">
                        <!-- TODO: update link -->
                        <a href="https://arxiv.org/abs/2508.17550" class="btn_pdf">pdf</a>
                        <a href="https://github.com/MAGICS-LAB/algo_emu" class="btn_code">code</a>
                    </div>
                </div>
                <!-- <p>We prove that a minimal Transformer with frozen weights emulates a broad class of algorithms by in-context prompting. 
                    We formalize two modes of in-context algorithm emulation. In the task-specific mode, for any continuous function, we show the existence of a single-head softmax attention layer whose forward pass reproduces machine learning algorithms.
                    In the prompt-programmable mode, we prove universality: a single fixed-weight two-layer softmax attention module emulates all algorithms from the task-specific class (i.e., each implementable by a single softmax attention) via only prompting.
                    Our key idea is to construct prompts that encode an algorithm's parameters into token representations, creating sharp dot-product gaps that force the softmax attention to follow the intended computation.
                    This construction requires no feed-forward layers and no parameter updates. All adaptation happens through the prompt alone. Numerical results corroborate our theory.
                    These findings forge a direct link between in-context learning and algorithmic emulation, and offer a simple mechanism for large Transformers to serve as prompt-programmable libraries of algorithms. 
                    They illuminate how GPT-style foundation models may swap algorithms via prompts alone, and establish a form of algorithmic universality in modern Transformer models.
                </p> -->
            </div>

            <div class="project-entry">
                <h3>Are Hallucinations Bad Estimations?</h3>
                <div class="project-callout">
                    <p class="callout-authors">
                        H. Liu<sup class="big-star">*</sup>, J. Y.-C. Hu<sup class="big-star">*</sup>, <strong>J. Y. Zhang</strong>, Z. Song, H. Liu<br>
                        In submission to ICLR 2026.
                    </p>
                    <div class="project-actions">
                        <!-- TODO: update link -->
                        <a href="https://www.arxiv.org/abs/2509.21473" class="btn_pdf">pdf</a>
                        <a href="https://github.com/MAGICS-LAB/hallucination" class="btn_code">code</a>
                    </div>
                </div>
                <!-- <p>We formalize hallucinations in generative models as failures to link an estimate to any plausible cause. 
                    Under this interpretation, we show that even loss-minimizing optimal estimators still hallucinate.
                    We confirm this with a general high probability lower bound on hallucinate rate for generic data distributions.
                    This reframes hallucination as structural misalignment between loss minimization and human-acceptable outputs, and hence estimation errors induced by miscalibration.
                    Experiments on coin aggregation, open-ended QA, and text-to-image support our theory.
                </p> -->
            </div>


            <div class="project-entry">
                <h3>WS-GRPO: Weakly-Supervised Group-Relative Policy Optimization</h3>
                <!-- <p><em>with <a href="https://cseweb.ucsd.edu/~jmcauley/">Prof. Julian McAuley</a></em></p> -->
                 <div class="project-callout">
                    <p class="callout-authors">
                        G. Mundada, R. Surana, <strong>J. Y. Zhang</strong>, X. Li, T. Yu, L. Yao, J. Shang, J. McAuley, J. Wu<br>
                        In submission to ICLR 2026.
                    </p>
                    <!-- <div class="project-actions">
                        <a href="https://arxiv.org/pdf/2508.17550" class="btn_pdf">pdf</a>
                        <a href="https://github.com/MAGICS-LAB/hallucination" class="btn_code">code</a>
                    </div> -->
                </div>
                <details class="project-abstract" data-collapsed>
                    <summary class="abstract-toggle">Read More</summary>
                    <div class="abstract-box">
                        <p>
                        Group-Relative Policy Optimization (GRPO) has emerged as an effective approach for training language models on complex reasoning tasks by normalizing rewards within groups of rollouts. 
                        However, GRPO's group-relative advantage estimation critically depends on dense step-wise reward signals throughout the reasoning process. 
                        In practice, obtaining such dense supervision requires expensive human annotations of intermediate reasoning steps or carefully designed step-wise reward functions. 
                        This creates a significant challenge specific to group-relative methods: while GRPO performs best with dense intermediate feedback, real-world scenarios often provide only sparse outcome supervision—such as final answer correctness or binary trajectory labels.
                        We propose Weakly-Supervised Group-Relative Policy Optimization (WS-GRPO), which addresses this unique limitation by learning to extract dense preference signals from sparse outcome supervision while preserving GRPO's group-relative normalization benefits. 
                        WS-GRPO operates in two phases: first, it trains a preference model to distinguish between successful and unsuccessful reasoning patterns using only trajectory-level outcomes; second, it leverages this learned preference model to provide step-wise weakly-supervised rewards that are combined with sparse terminal rewards during group-relative policy optimization. 
                        By treating consecutive partial trajectories as preference pairs, our method generates dense feedback signals that complement GRPO's group normalization mechanism without requiring step-by-step human annotations.
                        Theoretically, we provide comprehensive guarantees for WS-GRPO establishing preference model consistency under trajectory-level supervision, policy robustness to preference errors with controllable degradation rates, and generalization bounds that decompose error sources across policy learning, preference modeling, and their interaction. 
                        Our experiments on reasoning benchmarks demonstrate that WS-GRPO achieves competitive performance using only weak supervision, making group-relative policy optimization practical when detailed process supervision is limited.
                        </p>
                    </div>
                </details>
            </div>

            <div class="project-entry">
                <h3>SceneAlign: Aligning Multimodal Reasoning to Scene Graphs in Complex Visual Scenes</h3>
                <div class="project-callout">
                    <p class="callout-authors">
                        C. Wang, X. Li, <strong>J. Y. Zhang</strong>, J. Wu, C. Huang, L. Yao, J. McAuley, J. Shang<br>
                        In submission to ACL ARR Oct 2025.
                    </p>
                    <!-- <div class="project-actions">
                        <a href="https://arxiv.org/pdf/2508.17550" class="btn_pdf">pdf</a>
                        <a href="https://github.com/MAGICS-LAB/hallucination" class="btn_code">code</a>
                    </div> -->
                </div>
                <details class="project-abstract" data-collapsed>
                    <summary class="abstract-toggle">Read More</summary>
                    <div class="abstract-box">
                        <p>Multimodal large language models often struggle to reason faithfully in complex visual scenes, where multiple objects and relations require precise visual grounding at each reasoning step. 
                            Prior works improve reasoning via preference-based alignment with textual perturbations or answer edits, which encourage coherent reasoning but fail to address answer-grounding inconsistency errors such as hallucinated entities, mis-grounded relations, skipped steps, and over-specified reasoning in complex scenes. 
                            To address these limitations, we propose SceneAlign, a scene graph-guided preference alignment framework that enforces grounding consistency during reasoning alignment. Leveraging scene graphs as structured representations of objects and relations, SceneAlign identifies reasoning-critical nodes and perturbs them through four targeted strategies that mimic typical grounding failures.
                            The resulting grounded and perturbed rationales provide contrastive pairs for Direct Preference Optimization (DPO), steering models toward structure-faithful reasoning. 
                            Experiments across seven complex visual reasoning benchmarks show that SceneAlign consistently improves both answer accuracy and reasoning coherence, establishing a new paradigm for grounding-aware chain-of-thought supervision.
                        </p>
                    </div>
                </details>
            </div>

            <div class="project-entry">
                <h3>From Verifiable Rewards to Policy Learning: A Survey of Reinforcement Learning from Verifiable Rewards</h3>
                <div class="project-callout">
                    <p class="callout-authors">
                        G. Mundada, R. Surana, S. Yu, <strong>J. Y. Zhang</strong>, Z. Huang, Y. Xiong, X. Li, Y. Xia, R. Jain, C. Huang, N. L. Kuang, T. Yu, R. A. Rossi, D. Zhou, L. Yao, J. Shang, J. McAuley, J. Wu<br>
                        In submission to ACL ARR Oct 2025.
                    </p>
                    <!-- <div class="project-actions">
                        <a href="https://arxiv.org/pdf/2508.17550" class="btn_pdf">pdf</a>
                        <a href="https://github.com/MAGICS-LAB/hallucination" class="btn_code">code</a>
                    </div> -->
                </div>
                <details class="project-abstract" data-collapsed>
                    <summary class="abstract-toggle">Read More</summary>
                    <div class="abstract-box">
                        <p>Reinforcement Learning from Verifiable Rewards (RLVR) trains language model policies using feedback from verifiers such as exact-match checkers, execution-based tests, or constraint validators. 
                            Despite its success across mathematical reasoning, code generation, and instruction following, the RLVR literature still lacks unified terminology and systematic organization. 
                            This survey provides the first comprehensive systematization of RLVR methods. We formalize theoretical foundations and introduce three orthogonal taxonomies: 
                            (1) <em>when</em> verification is applied (terminal, step-level, trajectory-level); (2) <em>how</em> rewards are computed (human ground truth, model judges, programmatic oracles, environment feedback); and (3) <em>which</em> policy-learning (on-policy, hybrid). By establishing comprehensive taxonomies and consolidating evaluation resources, this survey provides a foundation for systematic comparison, informed method selection, and identification of open challenges in RLVR research.
                        </p>
                    </div>
                </details>
            </div>

            <!-- <div class="project-entry">
                <h3>Survey Paper on RL @ McAuley Lab, UCSD</h3>
                <p>TBD.</p>
            </div> -->

            <div class="project-entry">
                <h3>Realistic CDSS Drug Dosing with End-to-end Recurrent Q-learning for Dual Vasopressor Control</h3>
                <div class="project-callout">
                    <p class="callout-authors">
                        W. Y. Zou, J. Feng, A. Kalimouttou, <strong>J. Y. Zhang</strong>, C. W. Seymour, R. Pirracchio<br>
                        NeurIPS 2026 Workshop Learning from Time Series for Health.
                    </p>
                    <div class="project-actions">
                        <a href="https://arxiv.org/abs/2510.01508" class="btn_pdf">pdf</a>
                        <!-- <a href="https://github.com/JEN6YT/HCL" class="btn_code">code</a> -->
                    </div>
                </div>
                <!-- <p>Reinforcement learning (RL) applications in Clinical Decision Support Systems (CDSS) frequently encounter skepticism from practitioners regarding inoperable dosing decisions. We address this challenge with an end-to-end approach for learning optimal drug dosing and control policies for dual vasopressor administration in intensive care unit (ICU) patients with septic shock. 
                    For realistic drug dosing, we apply action space design that accommodates discrete, continuous, and directional dosing strategies in a system that combines offline conservative Q-learning with a novel recurrent modeling in a replay buffer to capture temporal dependencies in ICU time-series data. 
                    Our comparative analysis of norepinephrine dosing strategies across different action space formulations reveals that the designed action spaces improve interpretability and facilitate clinical adoption while preserving efficacy. 
                    Empirical results on eICUand MIMIC demonstrate that action space design profoundly influences learned behavioral policies. The proposed methods achieve improved patient outcomes of over 15% in survival improvement probability, while aligning with established clinical protocols.
                </p> -->
            </div>
            <div class="project-entry">
                <h3>Methane Emission Drivers and Baseline Calculations</h3>
                <div class="project-callout">
                    <p class="callout-authors">
                        <strong>J. Y. Zhang</strong>, B. A. faraz, L. A. Seco<br>
                        Preprint.
                    </p>
                    <div class="project-actions">
                        <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5045567" class="btn_pdf">pdf</a>
                        <a href="https://github.com/JEN6YT/Methane-Emission-Index" class="btn_code">code</a>
                    </div>
                </div>
                <!-- <p>Reinforcement learning (RL) applications in Clinical Decision Support Systems (CDSS) frequently encounter skepticism from practitioners regarding inoperable dosing decisions. We address this challenge with an end-to-end approach for learning optimal drug dosing and control policies for dual vasopressor administration in intensive care unit (ICU) patients with septic shock. 
                    For realistic drug dosing, we apply action space design that accommodates discrete, continuous, and directional dosing strategies in a system that combines offline conservative Q-learning with a novel recurrent modeling in a replay buffer to capture temporal dependencies in ICU time-series data. 
                    Our comparative analysis of norepinephrine dosing strategies across different action space formulations reveals that the designed action spaces improve interpretability and facilitate clinical adoption while preserving efficacy. 
                    Empirical results on eICUand MIMIC demonstrate that action space design profoundly influences learned behavioral policies. The proposed methods achieve improved patient outcomes of over 15% in survival improvement probability, while aligning with established clinical protocols.
                </p> -->
            </div>
        </section>

        <section id="cv">
            <div class="cv-wrapper">
                <!-- Sidebar navigation -->
                <nav class="sidebar">
                    <ul>
                        <li><a href="#basics">Basics</a></li>
                        <li><a href="#education">Education</a></li>
                        <li><a href="#work">Work</a></li>
                        <li><a href="#skills">Skills</a></li>
                        <li><a href="#interest">Interests</a></li>
                    </ul>
                </nav>
            
                <!-- CV Content -->
                <div class="cv-content">
                    <!-- Basics -->
                    <section id="basics" class="card">
                        <h2>Basics</h2>
                        <p><strong>Name:</strong> Yuntong (Jennifer) Zhang</p>
                        <p><strong>Label:</strong> 4th year Undergrad in Engineering Science at University of Toronto</p>
                        <p><strong>Email:</strong> 
                            <a href="mailto:jenniferyt.zhang@mail.utoronto.ca">jenniferyt.zhang@mail.utoronto.ca</a>
                        </p>
                        <p><strong>Summary:</strong> Interested in foundation models, RL, AI for society.</p>
                    </section>
            
                    <!-- Education -->
                    <section id="education" class="card">
                        <h2>Education</h2>
                        <div class="edu-item">
                            <div class="dates">
                                <span class="date-range">2021 - 2026</span>
                                <img src="assets/uoft.png" alt="UT" class="school-logo">
                            </div>
                            <div class="details">
                                <h3>BASc in Engineering Science</h3>
                                <p class="institution">University of Toronto</p>
                                <p class="desc">Graduated with High Distinction.</p>
                            </div>
                        </div>
                    </section>
            
                    <!-- Work -->
                    <section id="work" class="card">
                        <h2>Work</h2>
                        <div class="work-item">
                            <div class="dates">
                                <span class="date-range">Sep 2024 - May 2025</span>
                                <img src="assets/otpp.png" alt="Ontario Teachers' Pension Plan" class="company-logo">
                            </div>
                            <div class="details">
                                <h3>Total Fund Risk Analyst Intern</h3>
                                <p class="company">Ontario Teachers' Pension Plan</p>
                                <ul>
                                    <li>Developed novel statistical algorithms for financial risk calculations such as VaR and Expected Tail Loss.</li>
                                    <li>Researched and implemented LLM multi-agent systems for chatbot Question-Answering with in-house large financial datasets.</li>
                                    <li>Enhanced and maintained current risk calculation pipeline including data extraction, risk scenario generation, risk calculation, and risk
                                        dash-boarding.</li>
                                </ul>
                                <p class="skills">#Langchain, #Python, #SQL, #julia, #Csharp, #flask</p>
                            </div>
                        </div>
                        <hr/>
                        <div class="work-item">
                            <div class="dates">
                                <span class="date-range">May 2024 - Aug 2024</span>
                                <img src="assets/eqbank.png" alt="EQB" class="company-logo">
                            </div>
                            <div class="details">
                                <h3>Business Analyst Intern</h3>
                                <p class="company">EQ Bank</p>
                                <ul>
                                    <li>Conducted comprehensive analysis of the bank's financial reconciliation process, identifying key bottlenecks 
                                        and inefficiencies.</li>
                                    <li>Developed an automated reconciliation system for financial reporting, streamlining the workflow and reducing manual
                                        errors.</li>
                                    <li>Designed and implemented an ETL pipeline for financial data, improving data consistency and reducing data redundancy by 30%.</li>
                                </ul>
                                <p class="skills">#Python, #SQL, #Tableau, #VBA, #Excel</p>
                            </div>
                        </div>
                        <hr/>
                        <div class="work-item">
                            <div class="dates">
                                <span class="date-range">May 2023 - Dec 2023</span>
                                <img src="assets/nrc_cnrc.png" alt="nrc_cnrc" class="company-logo">
                            </div>
                            <div class="details">
                                <h3>Data Engineering Intern</h3>
                                <p class="company">National Reserach Council</p>
                                <ul>
                                    <li>Processed and analyzed large-scale geographical data to investigate factors contributing
                                        to fire incidents along railroads.</li>
                                    <li>Developed and compared machine learning models to predict active fire locations, achieving an accuracy of 89%.</li>
                                    <li>Created an interactive Tableau dashboard to visualize predicted high-risk fire areas along railroad routes, providing actionable
                                        insights for preventive measures.</li>
                                    <li>Collaborated with a cross-functional team to integrate machine learning models into the existing data pipeline, enhancing the overall
                                        efficiency of the data processing workflow.</li>
                                </ul>
                                <p class="skills">#scikit-learn, #Optuna, #geopandas, #Python, #Csharp, #Tableau, #SQL</p>
                            </div>
                        </div>
                    </section>
            
                    <!-- Skills -->
                    <section id="skills" class="card">
                        <div class="skills-container">
                            <div class="skills-content">
                                <h2>Skills</h2>
                                <ul class="skill-list">
                                    <li><strong>Programming:</strong> Python, SQL, C, C++, Julia, HTML, CSS, JavaScript</li>
                                    <li><strong>Deep Learning Programming:</strong> PyTorch, JAX</li>
                                    <li><strong>Machine Learning:</strong>
                                    <ul class="ml-list">
                                        <li>Deep Neural Networks</li>
                                        <li>Computer Vision</li>
                                        <li>Reinforcement Learning</li>
                                        <li>Self-supervised Learning</li>
                                        <li>Representation Learning</li>
                                        <li>Foundation Models</li>
                                    </ul>
                                    </li>
                                </ul>
                            </div>
                        
                            <!-- this empty div will get the image via JS -->
                            <div class="skills-image" id="skills-image">
                                <img src="assets/kugisaki.png" alt="Skills Image" class="skills-illustration"/>
                            </div>
                        </div>
                    </section>

                    <!-- Interests -->
                    <section id="interest" class="card">
                        <h2>Interests</h2>
                        <ul class="interest-list">
                            <li>Visual Reasoning of Large Foundation Models</li>
                            <li>Reinforcement Learning</li>
                            <li>AI Alignment</li>
                            <li>AI and Society</li>
                        </ul>
                    </section>
                </div>
            </div>
            
        </section>
    </main>

    <footer>
        <p>&copy; <span id="year"></span> Yuntong Zhang. All rights reserved.</p>
    </footer>


</body>

</html>